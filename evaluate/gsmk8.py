import torch
import re
from transformers import pipeline, AutoTokenizer,  AutoModelForCausalLM
from datasets import load_dataset
import numpy as np
import time
import os
#transfrmer = 3.40
os.environ['CUDA_VISIBLE_DEVICES'] = '2'  # é€‰æ‹© GPU

def extract_answer(text: str) -> int:
    """
    åœ¨æ–‡æœ¬ä¸­ä¼˜å…ˆæ£€ç´¢ä»¥ä¸‹ä¸¤ç§å½¢å¼ä¹‹ä¸€ï¼ˆåªè¿”å›åŒ¹é…åˆ°çš„ç¬¬ä¸€ä¸ªæ•°å­—ï¼‰ï¼š
      1) ä¸€ä¸ªæˆ–å¤šä¸ª '#' åé¢ç´§è·Ÿç©ºæ ¼ï¼ˆå¯é€‰ï¼‰å†ç´§è·Ÿæ•°å­—: e.g. ###18
      2) "The answer is " åé¢ç´§è·Ÿæ•°å­—ï¼ˆå¯é€‰å°¾éšå¥å·ï¼‰: e.g. The answer is 60.
    å¦‚æœåœ¨æ–‡æœ¬ä¸­æ‰¾ä¸åˆ°åŒ¹é…ï¼Œåˆ™è¿”å› Noneã€‚
    """
    # pattern åˆ†åˆ«ç”¨ä¸¤ä¸ªæ•è·ç»„ï¼šgroup(1) åŒ¹é… ###18 è¿™ç§å½¢å¼ï¼Œgroup(2) åŒ¹é… The answer is 60. è¿™ç§å½¢å¼
    pattern = re.compile(r"(#+\s*(\d+))|(?:The answer is\s*(\d+)\.?)")
    match = pattern.search(text)
    if match:
        # è‹¥æ˜¯åŒ¹é…åˆ° â€œ#+ æ•°å­—â€ï¼Œåˆ™æ•°å­—åœ¨ group(2)
        # è‹¥æ˜¯åŒ¹é…åˆ° â€œThe answer is æ•°å­—â€ï¼Œåˆ™æ•°å­—åœ¨ group(3)
        # æ³¨æ„è¿™é‡Œ group(1) æ•è·äº†æ•´æ®µ â€œ###18â€ï¼Œæˆ‘ä»¬çœŸæ­£æƒ³è¦çš„æ•°å­—éƒ¨åˆ†æ˜¯ group(2)
        # è€Œ "The answer is" çš„æ•°å­—åœ¨ group(3)
        number_str = match.group(2) or match.group(3)
        return int(number_str) if number_str else None
    return None

def test_llama3_base_on_gsm8k(num_samples=10):
    device = 0 if torch.cuda.is_available() else -1
    
    # ä»åŸºç¡€æ¨¡å‹åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
    
    # åŠ è½½å¾®è°ƒåçš„æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        "",
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    # åˆ›å»ºpipeline
    pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)
    
    # åŠ è½½ GSM8K æ•°æ®é›†
    dataset = load_dataset("openai/gsm8k", "main")
    test_data = dataset["test"]

    correct_count = 0
    wrong_questions = []  # å­˜å‚¨é”™è¯¯çš„é¢˜å·

    with open("improved_results.txt", "w", encoding="utf-8") as f:
        f.write("GSM8Kæµ‹è¯•ç»“æœ\n")
        f.write("=" * 50 + "\n")

    # æ”¹è¿›çš„æç¤ºæ¨¡æ¿
    prompt_template = """You are an expert at solving math word problems using careful step-by-step reasoning.

For each problem:
1. Understand what the problem is asking
2. Identify the variables and equations
3. Solve step by step, showing all calculations 
4. Double-check your answer
5. Format your final answer as ###[number] (just the number with no units)

Example 1:
Question: Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?
Answer: I need to find the total number of clips Natalia sold in April and May.

In April: 48 clips
In May: 48 Ã· 2 = 24 clips (half as many as April)

Total clips = April clips + May clips
Total clips = 48 + 24 = 72
###72
Example 2:
Question: Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?
Answer: I need to find how much Weng earned for 50 minutes of babysitting.

Hourly rate: $12
Rate per minute: $12 Ã· 60 = $0.2 per minute
Earnings for 50 minutes: 50 * $0.2 = $10

###10
Question: {question}
Answer:"""

    for i, sample in enumerate(test_data):
        question = sample["question"]
        correct_answer = extract_answer(sample["answer"])

        print(f"\nğŸ“Œ é—®é¢˜ {i+1}: {question}")
        
        prompt = prompt_template.format(question=question)

        # è°ƒæ•´æ¨¡å‹ç”Ÿæˆå‚æ•°
        response = pipe(
            prompt, 
            max_new_tokens=700,
            temperature=0.2,
            top_p=0.9,
            do_sample=True,
            return_full_text=False
        )
        model_output = response[0]['generated_text']

        # æå–ç­”æ¡ˆ
        model_answer = extract_answer(model_output)

        print(f"âœ… çœŸå®ç­”æ¡ˆ: {correct_answer}")
        print(f"ğŸ¤– ç”Ÿæˆçš„ç­”æ¡ˆ: {model_answer}")

        # è®¡ç®—æ­£ç¡®ç‡
        if model_answer is not None and correct_answer is not None and np.isclose(model_answer, correct_answer, atol=1e-2):
            correct_count += 1
        else:
            wrong_questions.append(i + 1)  # è®°å½•é¢˜å·

            # å°†é”™è¯¯é¢˜ç›®å†™å…¥ TXT æ–‡ä»¶
            with open("improved_wrong_answers.txt", "a", encoding="utf-8") as f:
                f.write(f"ğŸ“Œ é¢˜å·: {i+1}\n")
                f.write(f"ğŸ“– é¢˜ç›®: {question}\n")
                f.write(f"âœ… æ­£ç¡®ç­”æ¡ˆ: {correct_answer}\n")
                f.write(f"âŒ ç”Ÿæˆçš„ç­”æ¡ˆ: {model_answer}\n")
                f.write(f"ç”Ÿæˆå†…å®¹: {model_output}\n")
                f.write(f"æ­£ç¡®ç‡: {correct_count / (i + 1) * 100:.2f}%\n")
                f.write("-" * 50 + "\n")

        # è®¡ç®—å¹¶è¾“å‡ºå½“å‰æ­£ç¡®ç‡
        current_accuracy = (correct_count / (i + 1)) * 100
        print(f"ğŸ“Š å½“å‰æ­£ç¡®ç‡: {current_accuracy:.2f}%ï¼ˆ{correct_count}/{i+1}ï¼‰")

    # æ€»ç»“é”™è¯¯é¢˜ç›®
    if wrong_questions:
        print("\nğŸ“Œ é”™è¯¯çš„é¢˜ç›®ç¼–å·:", wrong_questions)
        print("ğŸ“‚ å·²å°†é”™è¯¯é¢˜ç›®ä¿å­˜åˆ° `improved_wrong_answers.txt`")
    
    print(f"æœ€ç»ˆå‡†ç¡®ç‡: {(correct_count / len(test_data)) * 100:.2f}%")
    

if __name__ == "__main__":
    start = time.time()
    test_llama3_base_on_gsm8k(num_samples=10)
    print(f"Total time: {time.time() - start:.2f}s")



# 


